{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faguilarc/RNN_v1/blob/master/Training_en_google_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "\n",
        "\n",
        "class AddressParser:\n",
        "\n",
        "    def __init__(self, model: NeuralParser, decoder: Decoder):\n",
        "        self.model = model\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def process_address(self, address_list: list):\n",
        "        probability_matrix = self.model.predict(address_list)\n",
        "\n",
        "        return self.decoder.decoder_to_first_address_model(probability_matrix, address_list)\n",
        "\n",
        "    def process_address_data_frame(self, address: DataFrame):\n",
        "        address_list = []\n",
        "        for i in address.index:\n",
        "            if len(str(address.iloc[i, 0])) != 0:\n",
        "                address_list.append(str(address.iloc[i, 0]))\n",
        "        probability_matrix = self.model.predict(address_list)\n",
        "\n",
        "        return self.decoder.decoder_to_first_address_model(probability_matrix, address_list)\n",
        "\n",
        "    @staticmethod\n",
        "    def to_xlsx(list_address: list, name_file='Results'):\n",
        "        if len(list_address) == 0:\n",
        "            raise NotImplementedError('The list should have at least one address')\n",
        "        elif isinstance(list_address[0], ClassifiedAddressOne):\n",
        "            AddressParser.__export_one(list_address, name_file=name_file, format='xlsx')\n",
        "        # FRANK poner la otra condicion de que si es instancia de ClassifiedAddresTwoAndThree\n",
        "\n",
        "    @staticmethod\n",
        "    def to_csv(list_address: list, name_file='Results'):\n",
        "        if len(list_address) == 0:\n",
        "            raise NotImplementedError('The list should have at least one address')\n",
        "        elif isinstance(list_address[0], ClassifiedAddressOne):\n",
        "            AddressParser.__export_one(list_address, name_file=name_file, format='csv')\n",
        "        # FRANK poner la otra condicion de que si es instancia de ClassifiedAddresTwoAndThree\n",
        "    @staticmethod\n",
        "    def __export_one(list_address: list, name_file='Results', format='xlsx'):\n",
        "        principal_street_list = []\n",
        "        first_side_street_list = []\n",
        "        second_side_street_list = []\n",
        "        locality_list = []\n",
        "        municipality_list = []\n",
        "        province_list = []\n",
        "        buildings_list = []\n",
        "        apartment_list = []\n",
        "        reserve_words_list = []\n",
        "\n",
        "        for address in list_address:\n",
        "            principal_street_list.append(' '.join(address.principal_street))\n",
        "            first_side_street_list.append(' '.join(address.first_side_street))\n",
        "            second_side_street_list.append(' '.join(address.second_side_street))\n",
        "            locality_list.append(' '.join(address.locality))\n",
        "            municipality_list.append(' '.join(address.municipality))\n",
        "            province_list.append(' '.join(address.province))\n",
        "            buildings_list.append(' '.join(address.building))\n",
        "            apartment_list.append(' '.join(address.apartment))\n",
        "            reserve_words_list.append(' '.join(address.reserve_word))\n",
        "\n",
        "        df = DataFrame({\n",
        "            'Principal Street': principal_street_list,\n",
        "            'First Side Street': first_side_street_list,\n",
        "            'Second Side Street': second_side_street_list,\n",
        "            'Building': buildings_list,\n",
        "            'Apartment': apartment_list,\n",
        "            'Locality': locality_list,\n",
        "            'Municipality': municipality_list,\n",
        "            'Province': province_list,\n",
        "            'Reserved Word': reserve_words_list,\n",
        "        })\n",
        "\n",
        "        if format == 'xlsx':\n",
        "            writer = pd.ExcelWriter(name_file + '.xlsx', engine='xlsxwriter')\n",
        "            df.to_excel(writer, index=False, sheet_name=name_file)\n",
        "            writer.save()\n",
        "        elif format == 'csv':\n",
        "            df.to_csv(name_file)\n",
        "        else:\n",
        "            raise NotImplementedError('This export format is not implemented')\n",
        "\n",
        "    # def process_address_three(self, address_list: list):\n",
        "    #     probability_matrix = self.model.predict(address_list)\n",
        "    #\n",
        "    #     return self.decoder.decoder_to_third_address_model(probability_matrix, address_list)\n",
        "\n",
        "    def process_address_two(self, address_list: list):\n",
        "        probability_matrix = self.model.predict(address_list)\n",
        "\n",
        "        return self.decoder.decoder_to_second_address_model(probability_matrix, address_list)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YVI-QyXttSp3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class AddressCleaner:\n",
        "\n",
        "    @staticmethod\n",
        "    def cleaner_method(method='custom_standardization'):\n",
        "        if method == 'custom_standardization':\n",
        "            return AddressCleaner.__custom_standardization\n",
        "        elif method == 'custom_standardization_v2':\n",
        "            return AddressCleaner.__custom_standardization_v2\n",
        "        else:\n",
        "            raise NotImplementedError('There is no such cleaning method')\n",
        "\n",
        "    @staticmethod\n",
        "    @tf.keras.utils.register_keras_serializable()\n",
        "    def __custom_standardization(input_string):\n",
        "        \"\"\" transforms words into lowercase and deletes punctuations \"\"\"\n",
        "\n",
        "        stripped_spanish = tf.strings.lower(input_string)\n",
        "\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'á', 'a')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'ä', 'a')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'Á', 'a')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'Ä', 'a')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'é', 'e')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'ë', 'e')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'É', 'e')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'Ë', 'e')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'í', 'i')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'ï', 'i')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'Í', 'i')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'Ï', 'i')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'ó', 'o')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'ö', 'o')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'Ó', 'o')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'Ö', 'o')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'ú', 'u')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'ü', 'u')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'Ú', 'u')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'Ü', 'u')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    ',', ' , ')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    ';', ' , ')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'ñ', 'n')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'entre', 'entre ')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    '#', ' # ')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    '%', ' % ')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    '&', ' y ')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    '/', ' / ')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'apt.', 'apt. ')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'apt', 'apt ')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish,\n",
        "                                                    'apartamento', 'apartamento ')\n",
        "        stripped_spanish = tf.strings.regex_replace(stripped_spanish, '[^a-zA-Z0-9 -/]', '')\n",
        "\n",
        "        output = tf.strings.regex_replace(\n",
        "            stripped_spanish, '[%s]' % re.escape(r\"\"\"!\"$&'()*+-.;<=>?@[]^_`{|}~\"\"\"), '')\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    @tf.keras.utils.register_keras_serializable()\n",
        "    def __custom_standardization_v2(input_string):\n",
        "        # Transforma toda la cadena a minúsculas\n",
        "        lower_str = input_string.lower()\n",
        "\n",
        "        # Quitar ½ y 1/2 en textos\n",
        "        spec_text = re.sub(r'½|1/2', ' ', lower_str)\n",
        "\n",
        "        # Reemplaza los caracteres y vocales especiales por espacios\n",
        "        char_spvow_off_str = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', spec_text)\n",
        "\n",
        "        # Quita cualquier caracter que no sea número o letra por espacio\n",
        "        clear_str = re.sub('[^0-9a-zA-Z]+', ' ', char_spvow_off_str)\n",
        "\n",
        "        return clear_str\n"
      ],
      "metadata": {
        "id": "TknOPEQJtTl3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from keras.utils import pad_sequences, to_categorical\n",
        "from pandas import DataFrame\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DataSetAdapter:\n",
        "    @staticmethod\n",
        "    def adapt(data_set: DataFrame, training_percentage: float, validation_percentage: float,\n",
        "              testing_percentage: float) -> DataSet:\n",
        "        if training_percentage + testing_percentage + validation_percentage > 1:\n",
        "            raise NotImplementedError('The sum of the percentages must correspond to 1')\n",
        "\n",
        "        data_set['Tag'] = data_set['Tag'].astype('category')\n",
        "        data_set['Tag_id'] = data_set['Tag'].cat.codes\n",
        "        id_to_category = pd.Series(data_set.Tag.values, index=data_set.Tag_id).to_dict()\n",
        "        n_tag = len(id_to_category)\n",
        "        input_dim = len(list(set(data_set['Word'].to_list()))) + 1\n",
        "        data_fillna = data_set.fillna(method='ffill', axis=0)\n",
        "        # Group by and collect columns\n",
        "        group_addres = data_fillna.groupby(\n",
        "            ['Sentence #'], as_index=False\n",
        "        )[['Word', 'Tag', 'Tag_id']].agg(lambda x: list(x))\n",
        "\n",
        "        features, targets = group_addres['Word'].tolist(), group_addres['Tag_id'].tolist()\n",
        "\n",
        "        train_features, test_features, train_targets, test_targets = train_test_split(\n",
        "            features, targets,\n",
        "            train_size=training_percentage + validation_percentage,\n",
        "            test_size=testing_percentage,\n",
        "            random_state=42,\n",
        "            shuffle=True,\n",
        "        )\n",
        "        train_features, val_features, train_targets, val_targets = train_test_split(\n",
        "            train_features, train_targets,\n",
        "            train_size=1 - validation_percentage / (training_percentage + validation_percentage),\n",
        "            test_size=validation_percentage / (training_percentage + validation_percentage),\n",
        "            random_state=42,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "        # Vocabulary is the list of all sentence of train set\n",
        "        vocabulary_word = DataSetAdapter.__get_sentence(train_features)\n",
        "\n",
        "        train_features_sentence = DataSetAdapter.__get_sentence(train_features)\n",
        "        test_features_sentence = DataSetAdapter.__get_sentence(test_features)\n",
        "        val_features_sentence = DataSetAdapter.__get_sentence(val_features)\n",
        "\n",
        "        max_len_word = max([len(s) for s in group_addres['Word'].tolist()])\n",
        "        max_len_characters = max(\n",
        "            [len(''.join(s)) for s in group_addres['Word'].tolist()])  # Se cuenta tambien los signos de puntacion.\n",
        "        max_len_trigram = max_len_characters\n",
        "\n",
        "        train_targets = DataSetAdapter.__get_tags(train_targets, n_tag, max_len_word)\n",
        "        test_targets = DataSetAdapter.__get_tags(test_targets, n_tag, max_len_word)\n",
        "        val_targets = DataSetAdapter.__get_tags(val_targets, n_tag, max_len_word)\n",
        "\n",
        "        return DataSet(vocabulary_word, max_len_characters, max_len_trigram, max_len_word, n_tag, id_to_category,\n",
        "                       input_dim,\n",
        "                       train_features_sentence, test_features_sentence, val_features_sentence, train_targets,\n",
        "                       test_targets, val_targets)\n",
        "\n",
        "    @staticmethod\n",
        "    def __get_sentence(words_list):\n",
        "        sentence_list = []\n",
        "        for feature in words_list:\n",
        "            sentence = ''\n",
        "            for word in feature:\n",
        "                sentence += word + ' '\n",
        "            sentence_list.append([sentence])\n",
        "        return sentence_list\n",
        "\n",
        "    @staticmethod\n",
        "    def __get_tags(data, n_tag, max_len, value=None):\n",
        "        if value is None:\n",
        "            value = n_tag - 1\n",
        "\n",
        "        tags = pad_sequences(data, maxlen=max_len, dtype='int32', padding='post', value=value)\n",
        "        tags = to_categorical(tags, num_classes=n_tag)\n",
        "\n",
        "        return tags\n"
      ],
      "metadata": {
        "id": "DE1nTd3ttYYQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randrange\n",
        "from random import choice\n",
        "import itertools as itt\n",
        "import math\n",
        "import random as rm\n",
        "\n",
        "from pandas import DataFrame\n",
        "# from spellchecker import SpellChecker\n",
        "# from fuzzywuzzy import fuzz\n",
        "\n",
        "\n",
        "class Generator:\n",
        "\n",
        "    def generate_non_standardization(self, components,probability):\n",
        "        reorder_components = components\n",
        "\n",
        "        if rm.randint(1, 100) <= probability:\n",
        "            permutations = list(itt.permutations(components))\n",
        "\n",
        "            amount_permutation = math.factorial(len(components)) - 1\n",
        "            reorder_components = permutations[rm.randint(0, amount_permutation)]\n",
        "\n",
        "        return reorder_components\n",
        "\n",
        "    def generate_prefix_randomly(self, list_prefix, probability):\n",
        "        if rm.randint(1, 100) <= probability:\n",
        "            prefix = list_prefix[rm.randint(0, len(list_prefix) - 1)]\n",
        "            return [[item, 'rw'] for item in prefix.split()]\n",
        "        return []\n",
        "\n",
        "    def generate_spelling_errors(self, word):\n",
        "        apply_value = randrange(100)\n",
        "        if apply_value > 65:\n",
        "            rand = randrange(100)\n",
        "            if rand < 25:\n",
        "                # Duplicate character\n",
        "                word = self.__duplicate_character(word)\n",
        "            elif rand < 50:\n",
        "                # Omit character\n",
        "                word = self.__omit_character(word)\n",
        "            elif rand < 75:\n",
        "                # Misspelling\n",
        "                word = self.__misspelling(word)\n",
        "            else:\n",
        "                # Replace similar character\n",
        "                word = self.__similar_character(word)\n",
        "\n",
        "        return word\n",
        "\n",
        "    def generate_data_frame(self, address_list, words_list, tags_list):\n",
        "        return DataFrame({\n",
        "            'Sentence #': address_list,\n",
        "            'Word': words_list,\n",
        "            'Tag': tags_list\n",
        "        })\n",
        "\n",
        "    def add_new_address(self, components, address_number, address_list, words_list, tags_list):\n",
        "        address_list.append('Sentence ' + str(address_number))\n",
        "\n",
        "        # breaking down\n",
        "        georeferential_elements_list = []\n",
        "        for element in components:\n",
        "            georeferential_elements_list += element\n",
        "\n",
        "        count = 0\n",
        "        var_aleatory = rm.randint(1, 6)\n",
        "        amount_errors = var_aleatory if rm.randint(0, 4) == 1 else 0\n",
        "        for compound_items in georeferential_elements_list:\n",
        "            if compound_items[0] != 'nan':\n",
        "                word = str(compound_items[0])\n",
        "                if amount_errors > 0 and rm.randint(1, 4) == 1:\n",
        "                    word = self.generate_spelling_errors(word)\n",
        "\n",
        "                words_list.append(word)\n",
        "                tags_list.append(str(compound_items[1]))\n",
        "\n",
        "                if count != 0 and len(words_list) == len(address_list) + 1:\n",
        "                    address_list.append(None)\n",
        "            count += 1\n",
        "\n",
        "    def is_empty(self, entity):\n",
        "\n",
        "        return len(entity) == 0 or len(entity.split()) == 0 or entity == 'nan' or entity is None\n",
        "\n",
        "    def generate_building_syntetic(self):\n",
        "        random_value = rm.randint(1, 100)\n",
        "        if random_value <= 45:\n",
        "            # only numbers\n",
        "            return str(rm.randint(101, 99999))\n",
        "        elif random_value <= 55:\n",
        "            # only letter\n",
        "            letters = ['A', 'B', 'C', 'D', 'F', 'G', 'H']\n",
        "            return letters[rm.randint(0, len(letters) - 1)]\n",
        "        else:\n",
        "            #  numbers and letters\n",
        "            letters = ['A', 'B', 'C', 'D', 'F', 'G', 'H']\n",
        "            letter = letters[rm.randint(0, len(letters) - 1)]\n",
        "            letter_position = rm.randint(0, 4)\n",
        "            number = str(rm.randint(101, 9999))\n",
        "\n",
        "            name = number[0: letter_position] + letter + number[letter_position:]\n",
        "            return name\n",
        "\n",
        "    def generate_apartment_syntetic(self):\n",
        "        random_value = rm.randint(1, 100)\n",
        "        if random_value <= 50:\n",
        "            # only numbers\n",
        "            return str(rm.randint(10, 101))\n",
        "        else:\n",
        "            #  numbers and letters\n",
        "            letters = ['A', 'B', 'C', 'D', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'R', 'S', 'T']\n",
        "            letter = letters[rm.randint(0, len(letters) - 1)]\n",
        "            letter_position = rm.randint(0, 4)\n",
        "            number = str(rm.randint(1, 90))\n",
        "\n",
        "            return number[0: letter_position] + letter + number[letter_position:]\n",
        "\n",
        "    def divide_equally(self,number):\n",
        "        part = number // 3\n",
        "        remainder = number % 3\n",
        "        if remainder == 0:\n",
        "            return part, part, part\n",
        "        elif remainder == 1:\n",
        "            return part + 1, part, part\n",
        "        else:\n",
        "            return part + 1, part , part + 1\n",
        "\n",
        "    def __duplicate_character(self, word):\n",
        "        char_index = randrange(len(word))\n",
        "        return word[:char_index] + word[char_index] + word[char_index:]\n",
        "\n",
        "    def __omit_character(self,word):\n",
        "        char_index = randrange(len(word))\n",
        "        return word[:char_index] + word[char_index + 1:]\n",
        "\n",
        "    def __misspelling(self,word):\n",
        "        # spell = SpellChecker()\n",
        "        # ret_word =''\n",
        "        # if word.lower() in spell:\n",
        "        #     suggestions = list(spell.candidates(word.lower()))\n",
        "        #     if len(suggestions) > 0:\n",
        "        #         new_word = choice(suggestions)\n",
        "        #         if fuzz.ratio(word.lower(), new_word) < 75:\n",
        "        #             ret_word = new_word\n",
        "        pass\n",
        "\n",
        "    def __similar_character(self,word):\n",
        "        ret_word = ''\n",
        "        char_index = randrange(len(word))\n",
        "        similar_chars = {'a': 'e', 'e': 'a', 'i': 'l', 'l': 'i', 'o': 'u', 'u': 'o', 'a': '@', '0': '@'}\n",
        "        if word[char_index].lower() in similar_chars:\n",
        "            new_char = similar_chars[word[char_index].lower()]\n",
        "            ret_word = word[:char_index] + new_char + word[char_index + 1:]\n",
        "        return ret_word\n",
        "\n"
      ],
      "metadata": {
        "id": "dKAz_uqateDZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from pandas import DataFrame\n",
        "import random as rm\n",
        "import itertools as itt\n",
        "\n",
        "class NoiseGenerator(Generator):\n",
        "    '''\n",
        "        Caracterizacion del modelo 1: (Componentes para permutar)\n",
        "\n",
        "        De acuerdo a la localidad, municipio, provincia se tiene los siguientes componentes: (Componentes Basicos)\n",
        "            [ prefijo + locaclidad]     [ prefijo + municipio]      [ prefijo + provincia]\n",
        "        De a cuerdo a las calles, esquinas, entrecalles, edificacion ( casa o edificio y numero de apartamento o piso) se pueden dividir en:\n",
        "\n",
        "            Tipo 1: Calle y entrecalles\n",
        "                [ prefijo + calle principal]    [ prefijo + calle secundaria + conjuncion + prefijo + calle secundaria] +-  [ prefijo + edificio + apat.]\n",
        "            Tipo 2: esquinas\n",
        "                2.1\n",
        "                    [ esq + prefijo calle + conjuncion + prefijo + calle]\n",
        "                2.2\n",
        "                    [ prefijo + calle + esq + prefijo + calle]\n",
        "            Tipo 2 Con variaciones: informacion de edificios, casas, piso, apartamento, etc.\n",
        "                2.1.1\n",
        "                    [ esq + prefijo calle + EDF + conjuncion + prefijo + calle]\n",
        "                    [ esq + prefijo calle + conjuncion + prefijo + calle + EDF]\n",
        "                2.2.1\n",
        "                    [ prefijo + calle + EDF + esq + prefijo + calle]\n",
        "                    [ prefijo + calle + esq + prefijo + calle + EDF]\n",
        "            Tipo 3: Solo calle principal\n",
        "                    [ prefijo + calle principal] [ prefijo + edificio + apat.]\n",
        "    '''\n",
        "\n",
        "    def generate_noise(self, data_set: DataFrame, address_amount=None):\n",
        "        print('Generate Noise II -- Type One')\n",
        "        address_number = 0\n",
        "        address_list = []\n",
        "        words_list = []\n",
        "        tags_list = []\n",
        "\n",
        "        for i in data_set.index:\n",
        "            if address_amount is not None and address_number == address_amount:\n",
        "                break\n",
        "            components = []\n",
        "\n",
        "            principal_street = str(data_set.iloc[i, 0])\n",
        "            first_side_street = str(data_set.iloc[i, 1])\n",
        "            second_side_street = str(data_set.iloc[i, 2])\n",
        "            locality = str(data_set.iloc[i, 3])\n",
        "            municipality = str(data_set.iloc[i, 4])\n",
        "            province = str(data_set.iloc[i, 5])\n",
        "\n",
        "            # RECORDAR LOS SUFIJOS: 5ta Ave. ; primera avenida\n",
        "            # Determinar si es tipo 1 o 2:\n",
        "            if not self.__is_empty(first_side_street) and not self.__is_empty(second_side_street):\n",
        "                #   Is type one\n",
        "                principal_street_prefix = super().generate_prefix_randomly(STREET_NAME_PREFIX, 50)\n",
        "                first_side_street_prefix = super().generate_prefix_randomly(STREET_NAME_PREFIX, 50)\n",
        "                second_side_street_prefix = super().generate_prefix_randomly(STREET_NAME_PREFIX, 50)\n",
        "\n",
        "                between_prefix = super().generate_prefix_randomly(BETWEEN_PREFIX, 100)\n",
        "                conjunction_prefix = [[\n",
        "                    'e' if (len(second_side_street_prefix) == 0 and second_side_street[0] == 'i') else 'y', 'rw']]\n",
        "\n",
        "                # create component\n",
        "                components.append(\n",
        "                    principal_street_prefix + [[item, 'principal_street'] for item in principal_street.split()]\n",
        "                )\n",
        "                flag = False\n",
        "                if rm.randint(1, 100) < 15:\n",
        "                    flag = True\n",
        "                    components.append(\n",
        "                        between_prefix + first_side_street_prefix + [[item, 'first_side_street'] for item in\n",
        "                                                                     first_side_street.split()] + conjunction_prefix +\n",
        "                        second_side_street_prefix + [[item, 'second_side_street'] for item in second_side_street.split()]\n",
        "                    )\n",
        "\n",
        "                if rm.randint(1, 100) <= 50:\n",
        "                    # Contain building\n",
        "                    identification_building = self.__generate_building_syntetic()\n",
        "                    identification_building_prefix = super().generate_prefix_randomly(BUILDING_PREFIX, 90)\n",
        "                    if rm.randint(1, 100) <= 40:\n",
        "                        components.append(\n",
        "                            identification_building_prefix + [[item, 'building'] for item in\n",
        "                                                              identification_building.split()]\n",
        "                        )\n",
        "                    else:\n",
        "                        # Contain apartment\n",
        "                        identification_apartment = self.__generate_apartment_syntetic()\n",
        "                        identification_apartment_prefix = super().generate_prefix_randomly(BUILDING_SUBDIVISION_PREFIX,\n",
        "                                                                                           100)\n",
        "\n",
        "                        components.append(\n",
        "                            identification_building_prefix + [[item, 'building'] for item in\n",
        "                                                              identification_building.split()] +\n",
        "                            identification_apartment_prefix + [[item, 'apartment'] for item\n",
        "                                                               in identification_apartment.split()]\n",
        "                        )\n",
        "                if not flag:\n",
        "                    components.append(\n",
        "                        between_prefix + first_side_street_prefix + [[item, 'first_side_street'] for item in\n",
        "                                                                     first_side_street.split()] + conjunction_prefix +\n",
        "                        second_side_street_prefix + [[item, 'second_side_street'] for item in\n",
        "                                                     second_side_street.split()]\n",
        "                    )\n",
        "            elif not self.__is_empty(first_side_street) or not self.__is_empty(second_side_street):\n",
        "                #   Is type 2\n",
        "                side_street = first_side_street if self.__is_empty(second_side_street) else second_side_street\n",
        "\n",
        "                principal_street_prefix = super().generate_prefix_randomly(STREET_NAME_PREFIX, 50)\n",
        "                side_street_prefix = super().generate_prefix_randomly(STREET_NAME_PREFIX, 50)\n",
        "\n",
        "                if rm.randint(1, 100) <= 50:\n",
        "                    # Is type 2.1\n",
        "                    corner_prefix = super().generate_prefix_randomly(CORNER_CONNECTOR_PREFIX, 100)\n",
        "                    conjunction_prefix = [['e' if (len(side_street_prefix) == 0 and side_street[0] == 'i') else 'y',\n",
        "                                           'rw']]\n",
        "\n",
        "                    if rm.randint(1, 100) <= 50:\n",
        "                        # Contain Building\n",
        "                        identification_building = self.__generate_building_syntetic()\n",
        "                        identification_building_prefix = super().generate_prefix_randomly(BUILDING_PREFIX, 90)\n",
        "\n",
        "                        if rm.randint(1, 100) <= 50:\n",
        "                            # Is type 2.1.1 left building\n",
        "                            components.append(\n",
        "                                corner_prefix + principal_street_prefix + [[item, 'principal_street'] for item in\n",
        "                                                                           principal_street.split()] +\n",
        "                                identification_building_prefix + [[item, 'building'] for item in\n",
        "                                                                  identification_building.split()] +\n",
        "                                conjunction_prefix + side_street_prefix + [[item, 'first_side_street'] for item in\n",
        "                                                                           side_street.split()]\n",
        "                            )\n",
        "                        else:\n",
        "                            # Is type 2.1.1 right building\n",
        "                            components.append(\n",
        "                                corner_prefix + principal_street_prefix + [[item, 'principal_street'] for item in\n",
        "                                                                           principal_street.split()] +\n",
        "                                conjunction_prefix + side_street_prefix + [[item, 'first_side_street'] for item in\n",
        "                                                                           side_street.split()] +\n",
        "                                identification_building_prefix + [[item, 'building'] for item in\n",
        "                                                                  identification_building.split()]\n",
        "                            )\n",
        "                    else:\n",
        "                        # Not contain building\n",
        "                        components.append(\n",
        "                            corner_prefix + principal_street_prefix + [[item, 'principal_street'] for item in\n",
        "                                                                       principal_street.split()] +\n",
        "                            conjunction_prefix + side_street_prefix + [[item, 'first_side_street'] for item in\n",
        "                                                                       side_street.split()]\n",
        "                        )\n",
        "                else:\n",
        "                    # Is type 2.2\n",
        "                    corner_prefix = super().generate_prefix_randomly(CORNER_CONNECTOR_PREFIX, 100)\n",
        "\n",
        "                    if rm.randint(1, 100) <= 50:\n",
        "                        # Contain Building\n",
        "                        identification_building = self.__generate_building_syntetic()\n",
        "                        identification_building_prefix = super().generate_prefix_randomly(BUILDING_PREFIX, 90)\n",
        "\n",
        "                        if rm.randint(1, 100) <= 50:\n",
        "                            # Is type 2.1.1 left building\n",
        "                            components.append(\n",
        "                                principal_street_prefix + [[item, 'principal_street'] for item in\n",
        "                                                           principal_street.split()] +\n",
        "                                identification_building_prefix + [[item, 'building'] for item in\n",
        "                                                                  identification_building.split()] +\n",
        "                                corner_prefix + side_street_prefix + [[item, 'first_side_street'] for item in\n",
        "                                                                      side_street.split()]\n",
        "                            )\n",
        "                        else:\n",
        "                            # Is type 2.1.1 right building\n",
        "                            components.append(\n",
        "                                principal_street_prefix + [[item, 'principal_street'] for item in\n",
        "                                                           principal_street.split()] +\n",
        "                                corner_prefix + side_street_prefix + [[item, 'first_side_street'] for item in\n",
        "                                                                      side_street.split()] +\n",
        "                                identification_building_prefix + [[item, 'building'] for item in\n",
        "                                                                  identification_building.split()]\n",
        "                            )\n",
        "                    else:\n",
        "                        # Not contain building\n",
        "                        components.append(\n",
        "                            principal_street_prefix + [[item, 'principal_street'] for item in\n",
        "                                                       principal_street.split()] +\n",
        "                            corner_prefix + side_street_prefix + [[item, 'first_side_street'] for item in\n",
        "                                                                  side_street.split()]\n",
        "                        )\n",
        "            else:\n",
        "                # Is type 3\n",
        "                principal_street_prefix = super().generate_prefix_randomly(STREET_NAME_PREFIX, 70)\n",
        "                components.append(\n",
        "                    principal_street_prefix + [[item, 'principal_street'] for item in principal_street.split()]\n",
        "                )\n",
        "                if rm.randint(1, 100) <= 50:\n",
        "                    # Contain building\n",
        "                    identification_building = self.__generate_building_syntetic()\n",
        "                    identification_building_prefix = super().generate_prefix_randomly(BUILDING_PREFIX, 90)\n",
        "                    if rm.randint(1, 100) <= 30:\n",
        "                        components.append(\n",
        "                            identification_building_prefix + [[item, 'building'] for item in\n",
        "                                                              identification_building.split()]\n",
        "                        )\n",
        "                    else:\n",
        "                        # Contain apartment\n",
        "                        identification_apartment = self.__generate_apartment_syntetic()\n",
        "                        identification_apartment_prefix = super().generate_prefix_randomly(BUILDING_SUBDIVISION_PREFIX,100)\n",
        "                        components.append(\n",
        "                            identification_building_prefix + [[item, 'building'] for item in\n",
        "                                                              identification_building.split()] +\n",
        "                            identification_apartment_prefix + [[item, 'apartment'] for item\n",
        "                                                               in identification_apartment.split()]\n",
        "                        )\n",
        "            # Components Basics\n",
        "            if not self.__is_empty(locality):\n",
        "                between_prefix = [[' , ', 'rw']] if rm.randint(1, 100) < 70 else []\n",
        "                locality_prefix = super().generate_prefix_randomly(LOCALITY_PREFIX, 35)\n",
        "                components.append(\n",
        "                    between_prefix + locality_prefix + [[item, 'locality'] for item in locality.split()]\n",
        "                )\n",
        "            if not self.__is_empty(municipality):\n",
        "                between_prefix = [[' , ', 'rw']] if rm.randint(1, 100) < 75 else []\n",
        "                municipality_prefix = super().generate_prefix_randomly(MUNICIPALITY_PREFIX, 8)\n",
        "                components.append(\n",
        "                    between_prefix + municipality_prefix + [[item, 'municipality'] for item in municipality.split()]\n",
        "                )\n",
        "            if not self.__is_empty(province):\n",
        "                province_prefix = super().generate_prefix_randomly(PROVINCE_PREFIX, 3)\n",
        "                between_prefix = [[' , ', 'rw']] if rm.randint(1, 100) < 85 else []\n",
        "                components.append(\n",
        "                    between_prefix + province_prefix + [[item, 'province'] for item in province.split()]\n",
        "                )\n",
        "\n",
        "            #  Permutación entre componentes.\n",
        "            if rm.randint(1, 100) <= 5:\n",
        "                components = super().generate_non_standardization(components)\n",
        "\n",
        "            address_number += 1\n",
        "            self.__add_new_address(components, address_number, address_list, words_list, tags_list)\n",
        "\n",
        "        # Adding real address\n",
        "        real_address = self.__add_real_address()\n",
        "        for address in real_address:\n",
        "            address_number += 1\n",
        "            self.__add_new_address(address, address_number, address_list, words_list, tags_list)\n",
        "\n",
        "        return self.__generate_data_frame(address_list, words_list, tags_list)\n",
        "\n",
        "    def __generate_data_frame(self, address_list, words_list, tags_list):\n",
        "        return DataFrame({\n",
        "            'Sentence #': address_list,\n",
        "            'Word': words_list,\n",
        "            'Tag': tags_list\n",
        "        })\n",
        "\n",
        "    def __add_new_address(self, components, address_number, address_list, words_list, tags_list):\n",
        "        address_list.append('Sentence ' + str(address_number))\n",
        "\n",
        "        # breaking down\n",
        "        georeferential_elements_list = []\n",
        "        for element in components:\n",
        "            georeferential_elements_list += element\n",
        "\n",
        "        count = 0\n",
        "        var_aleatory = rm.randint(1, 6)\n",
        "        amount_errors = var_aleatory if rm.randint(0, 4) == 1 else 0\n",
        "        for compound_items in georeferential_elements_list:\n",
        "            if compound_items[0] != 'nan':\n",
        "                word = str(compound_items[0])\n",
        "                if amount_errors > 0 and rm.randint(1, 4) == 1:\n",
        "                    word = super().generate_spelling_errors(word)\n",
        "\n",
        "                words_list.append(word)\n",
        "                tags_list.append(str(compound_items[1]))\n",
        "\n",
        "                if count != 0 and len(words_list) == len(address_list) + 1:\n",
        "                    address_list.append(None)\n",
        "            count += 1\n",
        "\n",
        "    def __is_empty(self, entity):\n",
        "\n",
        "        return len(entity) == 0 or len(entity.split()) == 0 or entity == 'nan' or entity is None\n",
        "\n",
        "    def __generate_building_syntetic(self):\n",
        "        random_value = rm.randint(1, 100)\n",
        "        if random_value <= 45:\n",
        "            # only numbers\n",
        "            return str(rm.randint(101, 99999))\n",
        "        elif random_value <= 55:\n",
        "            # only letter\n",
        "            letters = ['A', 'B', 'C', 'D', 'F', 'G', 'H']\n",
        "            return letters[rm.randint(0, len(letters) - 1)]\n",
        "        else:\n",
        "            #  numbers and letters\n",
        "            letters = ['A', 'B', 'C', 'D', 'F', 'G', 'H']\n",
        "            letter = letters[rm.randint(0, len(letters) - 1)]\n",
        "            letter_position = rm.randint(0, 4)\n",
        "            number = str(rm.randint(101, 9999))\n",
        "\n",
        "            name = number[0: letter_position] + letter + number[letter_position:]\n",
        "            return name\n",
        "\n",
        "    def __generate_apartment_syntetic(self):\n",
        "        random_value = rm.randint(1, 100)\n",
        "        if random_value <= 50:\n",
        "            # only numbers\n",
        "            return str(rm.randint(10, 101))\n",
        "        else:\n",
        "            #  numbers and letters\n",
        "            letters = ['A', 'B', 'C', 'D', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'R', 'S', 'T']\n",
        "            letter = letters[rm.randint(0, len(letters) - 1)]\n",
        "            letter_position = rm.randint(0, 4)\n",
        "            number = str(rm.randint(1, 90))\n",
        "\n",
        "            return number[0: letter_position] + letter + number[letter_position:]\n",
        "\n",
        "    def __add_real_address(self):\n",
        "        return [\n",
        "            [[['calle', 'rw'], ['30', 'principal_street'], ['959', 'building'], ['e', 'rw'], ['entre', 'rw'], ['avenida', 'rw'], ['26', 'first_side_street'], ['y', 'rw'], ['47', 'second_side_street'], ['Plaza', 'municipality'], ['de', 'municipality'], ['la', 'municipality'], ['Revolucion', 'municipality'], ['La', 'province'], ['Habana', 'province']]],\n",
        "            [[['ave', 'rw'], ['67', 'principal_street'], ['no', 'rw'], ['13613', 'building'], ['e', 'rw'], ['136', 'first_side_street'], ['y', 'rw'], ['138', 'second_side_street'], ['Marianao', 'municipality'], ['Marianao', 'municipality'], ['La', 'province'], ['HAbana', 'province'],]],\n",
        "            [[['calle', 'rw'], ['Gomez', 'principal_street'], ['2', 'building'], ['E', 'building'], ['entre', 'rw'], ['calle', 'rw'], ['Marti', 'first_side_street'], ['y', 'rw'], ['calle', 'rw'], ['Washington', 'second_side_street'], ['reparto', 'rw'], ['barrio', 'rw'], ['Azul', 'locality'], ['Arroyo', 'municipality'], ['Naranjo', 'municipality'], ['La', 'province'], ['Habana', 'province']]],\n",
        "            [[['San', 'principal_street'], ['Juan', 'principal_street'], ['de', 'principal_street'], ['Dios', 'principal_street'], ['Edif', 'rw'], ['108', 'building'], ['apto', 'rw'], ['15', 'apartment'], ['entre', 'rw'], ['aguacate', 'first_side_street'], ['y', 'rw'], ['Compostela', 'second_side_street'], ['La', 'municipality'], ['Habana', 'municipality'], ['Vieja', 'municipality'], ['La', 'province'], ['Habana', 'province']]],\n",
        "            [[['avenida', 'rw'], ['del', 'principal_street'], ['sur', 'principal_street'], ['entre', 'rw'], ['primelles', 'first_side_street'], ['y', 'rw'], ['Lazada', 'second_side_street'], [',', 'rw'], ['Norte', 'locality'], ['III', 'locality'], [',', 'rw'], ['CERRO', 'municipality'], [',', 'rw'], ['LA', 'province'], ['HABANA', 'province'],]],\n",
        "            [[['San', 'principal_street'], ['Juan', 'principal_street'], ['DE', 'principal_street'], ['dios', 'principal_street'], ['entre', 'rw'], ['aguacate', 'first_side_street'], ['y', 'rw'], ['compostela', 'second_side_street'], [',', 'rw'], ['La', 'municipality'], ['Habana', 'municipality'], ['Vieja', 'municipality'], [',', 'rw'], ['La', 'province'], ['Habana', 'province'],]],\n",
        "            [[['27', 'principal_street'], ['b', 'principal_street'], ['entre', 'rw'], ['230', 'first_side_street'], ['y', 'rw'], ['234', 'second_side_street'], [',', 'rw'], ['La', 'locality'], ['Coronela', 'locality'], [',', 'rw'], ['La', 'municipality'], ['Lisa', 'municipality'], [',', 'rw'], ['La', 'province'], ['Habana', 'province'],]],\n",
        "            [[['calle', 'rw'], ['REYES', 'principal_street'], ['entre', 'rw'], ['c', 'first_side_street'], ['y', 'rw'], ['calle', 'rw'], ['Altarriba', 'second_side_street'], ['Edificio', 'rw'], ['319', 'building'], ['Apto', 'rw'], ['9', 'apartment'], ['Barrio', 'rw'], ['Lawton', 'locality'], ['Diez', 'municipality'], ['de', 'municipality'], ['Octubre', 'municipality'], ['La', 'province'], ['Habana', 'province'],]],\n",
        "            [[['calle', 'rw'], ['real', 'principal_street'], ['#', 'rw'], ['360', 'building'], ['poblado', 'rw'], ['bacuranao', 'locality'], [',', 'rw'], ['guanabacoa', 'municipality'], [',', 'rw'], ['La', 'province'], ['Habana', 'province'],]],\n",
        "            [[['calle', 'rw'], ['82', 'principal_street'], ['E', 'rw'], ['/', 'rw'], ['calle', 'rw'], ['5D', 'first_side_street'], ['y', 'rw'], ['calle', 'rw'], ['7', 'second_side_street'], ['Edificio', 'rw'], ['iacc', 'building'], ['#', 'rw'], ['5d14', 'building'], [',', 'rw'], ['apto', 'rw'], ['8', 'apartment'], ['repto', 'rw'], ['villa', 'locality'], ['panamericana', 'locality'], [',', 'rw'], ['La', 'municipality'], ['Habana', 'municipality'], ['del', 'municipality'], ['Este', 'municipality'], [',', 'rw'], ['La', 'province'], ['Habana', 'province'],]],\n",
        "            [[['calle', 'rw'], ['5ta', 'principal_street'], ['num', 'rw'], ['5800', 'building'], ['Bajo', 'rw'], ['entre', 'rw'], ['calle', 'rw'], ['b', 'first_side_street'], ['y', 'rw'], ['calle', 'rw'], ['c', 'second_side_street'], [',', 'rw'], ['SAN', 'municipality'], ['MIGUEL', 'municipality'], ['DEL', 'municipality'], ['PADRON', 'municipality'], [',', 'rw'], ['LA', 'province'], ['HABANA', 'province'],]],\n",
        "            [[['calle', 'rw'], ['A', 'principal_street'], ['no', 'rw'], ['48', 'building'], ['y', 'rw'], ['apto', 'rw'], ['1', 'apartment'], ['e', 'rw'], ['entre', 'rw'], ['calle', 'rw'], ['pinar', 'first_side_street'], ['del', 'first_side_street'], ['rio', 'first_side_street'], ['y', 'rw'], ['calle', 'rw'], ['woodberry', 'second_side_street'], ['reparto', 'rw'], ['callejas', 'locality'], ['ARROYO', 'municipality'], ['NARANJO', 'municipality'], ['LA', 'province'], ['HABANA', 'province']]],\n",
        "            [[['calle', 'rw'], ['7ma', 'principal_street'], ['e', 'rw'], ['entre', 'rw'], ['calle', 'rw'], ['l', 'first_side_street'], ['y', 'rw'], ['calle', 'rw'], ['10', 'second_side_street'], ['edificio', 'rw'], ['10103', 'building'], ['apto', 'rw'], ['23', 'apartment'], ['reparto', 'rw'], ['Altahabana', 'locality'], ['BOYEROS', 'municipality'], ['LA', 'province'], ['HABANA', 'province'],]],\n",
        "            [[['avenida', 'rw'], ['27', 'principal_street'], ['b', 'principal_street'], ['entre', 'rw'], ['calle', 'rw'], ['230', 'first_side_street'], ['y', 'rw'], ['calle', 'rw'], ['234', 'second_side_street'], ['edificio', 'rw'], ['22', 'building'], ['apto', 'rw'], ['18', 'apartment'], ['reparto', 'rw'], ['la', 'locality'], ['coronela', 'locality'], ['la', 'municipality'], ['lisa', 'municipality'], ['La', 'province'], ['Habana', 'province'],]],\n",
        "            [[['avenida', 'rw'], ['27', 'principal_street'], ['b', 'principal_street'], ['e', 'rw'], ['entre', 'rw'], ['calle', 'rw'], ['230', 'first_side_street'], ['y', 'rw'], ['calle', 'rw'], ['234', 'second_side_street'], ['Edificio', 'rw'], ['10', 'building'], ['Apto', 'rw'], ['19', 'apartment'], ['reparto', 'rw'], ['la', 'locality'], ['coronela', 'locality'], ['la', 'municipality'], ['lisa', 'municipality'], ['La', 'province'], ['Habana', 'province'],]],\n",
        "            [[['calle', 'rw'], ['100', 'principal_street'], ['5907', 'building'], ['bajos', 'rw'], ['entre', 'rw'], ['ave', 'rw'], ['59', 'first_side_street'], ['y', 'rw'], ['61', 'second_side_street'], ['Marianao', 'municipality'], ['La', 'province'], ['HABANA', 'province'],]],\n",
        "            [[['Cisneros', 'principal_street'], ['21', 'building'], ['Altos', 'rw'], ['e', 'rw'], ['entre', 'rw'], ['arnao', 'first_side_street'], ['y', 'rw'], ['cortez', 'second_side_street'], ['ARROYO', 'municipality'], ['NARANJO', 'municipality'], ['LA', 'province'], ['HABANA', 'province'],]],\n",
        "            [[['avenida', 'rw'], ['47', 'principal_street'], ['4003', 'building'], ['e', 'rw'], ['entre', 'rw'], ['calle', 'rw'], ['40', 'first_side_street'], ['y', 'rw'], ['avenida', 'rw'], ['41', 'second_side_street'], ['reparto', 'rw'], ['kohly', 'locality'], ['playa', 'municipality'], ['la', 'province'], ['habana', 'province'],]],\n",
        "            [[['calle', 'rw'], ['59', 'principal_street'], ['no', 'rw'], ['10814A', 'building'], ['e', 'rw'], ['entre', 'rw'], ['108', 'first_side_street'], ['y', 'rw'], ['110', 'second_side_street'], ['Apto', 'rw'], ['3', 'apartment'], ['marianao', 'municipality'], ['la', 'province'], ['habana', 'province']]],\n",
        "        ]\n"
      ],
      "metadata": {
        "id": "7tBYzRCAtjGj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import DataFrame\n",
        "import random as rm\n",
        "\n",
        "class NoiseGeneratorModelTwo(Generator):\n",
        "\n",
        "    def generate_noise(self, data_set: DataFrame,type=None, address_amount=None):\n",
        "        '''\n",
        "\n",
        "                :param data_set: this is the corpus for generate\n",
        "\n",
        "                :param type: you can specify which kind of addresses examples you want to use it\n",
        "                        example for using correct addresses  = \"ce\"\n",
        "                        example for using almost correct addresses  = \"ace\"\n",
        "                        example for using uncorrect addresses  = \"uce\"\n",
        "                        example for using uncorrect addresses  = \"eq\"\n",
        "\n",
        "\n",
        "                :param address_amount: this value indicates the amount of addresses this method will use\n",
        "\n",
        "                :return: DataFrame\n",
        "                '''\n",
        "\n",
        "        self.address_amount = address_amount\n",
        "        self.data = data_set\n",
        "        self.type = type\n",
        "\n",
        "        if self.type == 'ce':\n",
        "            address_number = 0\n",
        "            address_list = []\n",
        "            words_list = []\n",
        "            tag_list = []\n",
        "            for index in range(self.address_amount):\n",
        "                components = self.__generate_correct_example_type_two()\n",
        "                address_number += 1\n",
        "                super().add_new_address(components, address_number, address_list, words_list, tag_list)\n",
        "\n",
        "            return super().generate_data_frame(address_list, words_list, tag_list)\n",
        "        elif type == 'ace':\n",
        "            address_number = 0\n",
        "            address_list = []\n",
        "            words_list = []\n",
        "            tag_list = []\n",
        "\n",
        "            for index in range(self.address_amount):\n",
        "                components = self.__generate_almost_correct_examples_type_two()\n",
        "                address_number += 1\n",
        "                super().add_new_address(components, address_number, address_list, words_list, tag_list)\n",
        "\n",
        "            return super().generate_data_frame(address_list, words_list, tag_list)\n",
        "        elif type == 'uce':\n",
        "            address_number = 0\n",
        "            address_list = []\n",
        "            words_list = []\n",
        "            tag_list = []\n",
        "\n",
        "            for index in range(self.address_amount):\n",
        "                components = self.__generate_uncorrect_examples_type_two()\n",
        "                address_number += 1\n",
        "                super().add_new_address(components, address_number, address_list, words_list, tag_list)\n",
        "\n",
        "            return super().generate_data_frame(address_list, words_list, tag_list)\n",
        "        elif type == 'eq':\n",
        "            return self.__generate_equilibrated_examples_type_two()\n",
        "        else:\n",
        "            raise NotImplementedError('There is no such kind of example')\n",
        "\n",
        "\n",
        "    def __generate_equilibrated_examples_type_two(self):\n",
        "\n",
        "        print('Generate_random_noise_type_two')\n",
        "        address_number = 0\n",
        "        address_list = []\n",
        "        words_list = []\n",
        "        tag_list = []\n",
        "\n",
        "        correct,almost_correct,uncorrect = super().divide_equally(self.address_amount)\n",
        "\n",
        "\n",
        "        for index in range(correct):\n",
        "            components = self.__generate_correct_example_type_two()\n",
        "            address_number += 1\n",
        "            super().add_new_address(components, address_number, address_list, words_list, tag_list)\n",
        "\n",
        "        for index in range(almost_correct):\n",
        "            components = self.__generate_almost_correct_examples_type_two()\n",
        "            address_number += 1\n",
        "            super().add_new_address(components, address_number, address_list, words_list, tag_list)\n",
        "\n",
        "        for index in range(uncorrect):\n",
        "            components = self.__generate_uncorrect_examples_type_two()\n",
        "            address_number += 1\n",
        "            super().add_new_address(components, address_number, address_list, words_list, tag_list)\n",
        "\n",
        "\n",
        "        return super().generate_data_frame(address_list, words_list, tag_list)\n",
        "\n",
        "    def __generate_correct_example_type_two(self):\n",
        "        components = []\n",
        "\n",
        "        building = super().generate_building_syntetic()\n",
        "        locality = str(rm.choice(self.data['locality']))\n",
        "        municipality = str(rm.choice(self.data['municipality']))\n",
        "        province = str(rm.choice(self.data['province']))\n",
        "\n",
        "        building_form = super().generate_prefix_randomly(BUILDING_PREFIX_CORRECT, 100)\n",
        "        number_form = super().generate_prefix_randomly(PROPERTY_PREFIX_CORRECT, 100)\n",
        "        apartment_form = super().generate_prefix_randomly(APARTMENT_PREFIX_CORRECT, 100)\n",
        "        zone_form = super().generate_prefix_randomly(ZONE_PREFIX_CORRECT, 100)\n",
        "        locality_form = super().generate_prefix_randomly(LOCALITY_PREFIX_CORRECT, 100)\n",
        "        municipality_prefix = super().generate_prefix_randomly(MUNICIPALITY_PREFIX_CORRECT, 100)\n",
        "        province_prefix = super().generate_prefix_randomly(PROVINCE_PREFIX_CORRECT, 100)\n",
        "\n",
        "        # For creating the first component we have to split itself in three sub-components, if the name of the building is alphanum\n",
        "        # For example: Edif 456 o 34B\n",
        "        # [reserved word for building]+ [reserved word for number] + [building]\n",
        "        # then a random number is used to decide whether or not the number component appears\n",
        "        # And if it's alpha would be like:\n",
        "        # [reserved word for building] + [name]\n",
        "\n",
        "        is_name = True if building.isalpha() else False\n",
        "\n",
        "        # CREATING COMPONENT 1  --- [building_form],[number_form],[building] ---\n",
        "        if is_name:\n",
        "            components.append(\n",
        "                building_form + [[item, 'building'] for item in\n",
        "                                 building.split()]\n",
        "            )\n",
        "        else:\n",
        "            # Using the random number mentioned above\n",
        "            # Add property prefix\n",
        "            add_num_component = True if rm.randint(0, 100) > 50 else False\n",
        "            if add_num_component:\n",
        "                components.append(\n",
        "                    building_form + number_form + [[item, 'building'] for item in\n",
        "                                                   building.split()]\n",
        "                )\n",
        "            else:\n",
        "                components.append(\n",
        "                    building_form + [[item, 'building'] for item in\n",
        "                                     building.split()]\n",
        "                )\n",
        "\n",
        "        # CREATING COMPONENT 2  --- [apartment_form],[number_form],[apartment] ---\n",
        "        # This component is optional\n",
        "\n",
        "        add_apart_component = rm.randint(0, 100)\n",
        "        if add_apart_component > 50:\n",
        "            apartment_num = super().generate_apartment_syntetic()\n",
        "\n",
        "            add_num_component = True if rm.randint(0, 100) > 50 else False\n",
        "            if add_num_component:\n",
        "                components.append(\n",
        "                    apartment_form + number_form + [[item, 'apartment'] for item in\n",
        "                                                    apartment_num.split()]\n",
        "                )\n",
        "            else:\n",
        "                components.append(\n",
        "                    apartment_form + [[item, 'apartment'] for item in\n",
        "                                      apartment_num.split()]\n",
        "                )\n",
        "\n",
        "        # CREATING COMPONENT 3  --- [locality_form],[locality] ---\n",
        "        if locality != '':\n",
        "            loc_aux = ''\n",
        "            loc_zone = ''\n",
        "            if locality.lower().find('alamar') != -1:\n",
        "                spl_loc = locality.split()\n",
        "                if len(spl_loc) != 1:\n",
        "                    loc_aux = 'Alamar'\n",
        "                    for word in spl_loc:\n",
        "                        if word.lower() != 'alamar':\n",
        "                            loc_zone += word + ' '\n",
        "\n",
        "                    # Adding the zone\n",
        "                    if loc_zone.lower().find('micro') != -1:\n",
        "                        if rm.randint(0, 50) > 25:\n",
        "                            components.append(\n",
        "                                locality_form + [[item, 'locality'] for item in loc_aux.split()] + zone_form + [\n",
        "                                    [item, 'locality'] for item in\n",
        "                                    loc_zone.split()]\n",
        "                            )\n",
        "                        else:\n",
        "                            components.append(\n",
        "                                zone_form + [[item, 'locality'] for item in loc_zone.split()]\n",
        "                                + locality_form + [[item, 'locality'] for item in loc_aux.split()]\n",
        "                            )\n",
        "\n",
        "                    else:\n",
        "                        add_num_component = True if rm.randint(0, 100) > 50 else False\n",
        "                        if add_num_component:\n",
        "                            if rm.randint(0, 50) > 25:\n",
        "                                components.append(\n",
        "                                    locality_form + [[item, 'locality'] for item in loc_aux.split()]\n",
        "                                    + zone_form + number_form + [[item, 'locality'] for item in\n",
        "                                                                 loc_zone.split()]\n",
        "                                )\n",
        "                            else:\n",
        "                                components.append(\n",
        "                                    zone_form + number_form + [[item, 'locality'] for item in loc_zone.split()]\n",
        "                                    + locality_form + [[item, 'locality'] for item in loc_aux.split()]\n",
        "                                )\n",
        "\n",
        "                        else:\n",
        "                            if rm.randint(0, 50) > 25:\n",
        "                                components.append(\n",
        "                                    locality_form + [[item, 'locality'] for item in loc_aux.split()] + zone_form + [\n",
        "                                        [item, 'locality'] for item in\n",
        "                                        loc_zone.split()]\n",
        "                                )\n",
        "                            else:\n",
        "                                components.append(\n",
        "                                    zone_form + [[item, 'locality'] for item in loc_zone.split()]\n",
        "                                    + locality_form + [[item, 'locality'] for item in loc_aux.split()]\n",
        "                                )\n",
        "                else:\n",
        "                    components.append(\n",
        "                        locality_form + [[item, 'locality'] for item in locality.split()]\n",
        "                    )\n",
        "            else:\n",
        "                components.append(\n",
        "                    locality_form + [[item, 'locality'] for item in locality.split()]\n",
        "                )\n",
        "\n",
        "        # CREATING COMPONENT 4  --- [municipality_form],[municipality] ---\n",
        "        if len(municipality) != 0 or not super().is_empty(municipality):\n",
        "            components.append(\n",
        "                municipality_prefix + [[item, 'municipality'] for item in municipality.split()]\n",
        "            )\n",
        "\n",
        "        # CREATING COMPONENT 5  --- [province_form],[province] ---\n",
        "        if len(province) != 0 or not super().is_empty(province):\n",
        "            components.append(\n",
        "                province_prefix + [[item, 'province'] for item in province.split()]\n",
        "            )\n",
        "\n",
        "            #  Permutación entre componentes.\n",
        "            components = super().generate_non_standardization(components, 30)\n",
        "\n",
        "        return components\n",
        "\n",
        "    def __generate_almost_correct_examples_type_two(self):\n",
        "        components = []\n",
        "\n",
        "        building = super().generate_building_syntetic()\n",
        "        locality = str(rm.choice(self.data['locality']))\n",
        "        municipality = str(rm.choice(self.data['municipality']))\n",
        "        province = str(rm.choice(self.data['province']))\n",
        "\n",
        "        building_form = super().generate_prefix_randomly(BUILDING_PREFIX, 70)\n",
        "        number_form = super().generate_prefix_randomly(PROPERTY_PREFIX, 70)\n",
        "        apartment_form = super().generate_prefix_randomly(APARTMENT_PREFIX, 70)\n",
        "        zone_form = super().generate_prefix_randomly(ZONE_PREFIX, 60)\n",
        "        locality_form = super().generate_prefix_randomly(LOCALITY_PREFIX, 60)\n",
        "        municipality_prefix = super().generate_prefix_randomly(MUNICIPALITY_PREFIX, 60)\n",
        "        province_prefix = super().generate_prefix_randomly(PROVINCE_PREFIX, 40)\n",
        "\n",
        "        # For creating the first component we have to split itself in three sub-components, if the name of the building is alphanum\n",
        "        # For example: Edif 456 o 34B\n",
        "        # [reserved word for building]+ [reserved word for number] + [building]\n",
        "        # then a random number is used to decide whether or not the number component appears\n",
        "        # And if it's alpha would be like:\n",
        "        # [reserved word for building] + [name]\n",
        "\n",
        "        is_name = True if building.isalpha() else False\n",
        "\n",
        "        # CREATING COMPONENT 1  --- [building_form],[number_form],[building] ---\n",
        "        if is_name:\n",
        "            components.append(\n",
        "                building_form + [[item, 'building'] for item in\n",
        "                                 building.split()]\n",
        "            )\n",
        "        else:\n",
        "            # Using the random number mentioned above\n",
        "            # Add property prefix\n",
        "            add_num_component = True if rm.randint(0, 100) > 50 else False\n",
        "            if add_num_component:\n",
        "                components.append(\n",
        "                    building_form + number_form + [[item, 'building'] for item in\n",
        "                                                   building.split()]\n",
        "                )\n",
        "            else:\n",
        "                components.append(\n",
        "                    building_form + [[item, 'building'] for item in\n",
        "                                     building.split()]\n",
        "                )\n",
        "\n",
        "        # CREATING COMPONENT 2  --- [apartment_form],[number_form],[apartment] ---\n",
        "        # This component is optional\n",
        "\n",
        "        add_apart_component = rm.randint(0, 100)\n",
        "        if add_apart_component > 50:\n",
        "            apartment_num = super().generate_apartment_syntetic()\n",
        "\n",
        "            # para variante num, letra+num\n",
        "            add_num_component = True if rm.randint(0, 100) > 50 else False\n",
        "            if add_num_component:\n",
        "                components.append(\n",
        "                    apartment_form + number_form + [[item, 'apartment'] for item in\n",
        "                                                    apartment_num.split()]\n",
        "                )\n",
        "            else:\n",
        "                components.append(\n",
        "                    apartment_form + [[item, 'apartment'] for item in\n",
        "                                      apartment_num.split()]\n",
        "                )\n",
        "\n",
        "        # CREATING COMPONENT 3  --- [locality_form],[locality] ---\n",
        "        if locality != '':\n",
        "            loc_aux = ''\n",
        "            loc_zone = ''\n",
        "            if locality.lower().find('alamar') != -1:\n",
        "                spl_loc = locality.split()\n",
        "                if len(spl_loc) != 1:\n",
        "                    loc_aux = 'Alamar'\n",
        "                    for word in spl_loc:\n",
        "                        if word.lower() != 'alamar':\n",
        "                            loc_zone += word + ' '\n",
        "\n",
        "                    # Adding the zone\n",
        "                    if loc_zone.lower().find('micro') != -1:\n",
        "                        if rm.randint(0, 50) > 25:\n",
        "                            components.append(\n",
        "                                locality_form + [[item, 'locality'] for item in loc_aux.split()] + zone_form + [\n",
        "                                    [item, 'locality'] for item in\n",
        "                                    loc_zone.split()]\n",
        "                            )\n",
        "                        else:\n",
        "                            components.append(\n",
        "                                zone_form + [[item, 'locality'] for item in loc_zone.split()]\n",
        "                                + locality_form + [[item, 'locality'] for item in loc_aux.split()]\n",
        "                            )\n",
        "\n",
        "                    else:\n",
        "                        add_num_component = True if rm.randint(0, 100) > 50 else False\n",
        "                        if add_num_component:\n",
        "                            if rm.randint(0, 50) > 25:\n",
        "                                components.append(\n",
        "                                    locality_form + [[item, 'locality'] for item in loc_aux.split()]\n",
        "                                    + zone_form + number_form + [[item, 'locality'] for item in\n",
        "                                                                 loc_zone.split()]\n",
        "                                )\n",
        "                            else:\n",
        "                                components.append(\n",
        "                                    zone_form + number_form + [[item, 'locality'] for item in loc_zone.split()]\n",
        "                                    + locality_form + [[item, 'locality'] for item in loc_aux.split()]\n",
        "                                )\n",
        "\n",
        "                        else:\n",
        "                            if rm.randint(0, 50) > 25:\n",
        "                                components.append(\n",
        "                                    locality_form + [[item, 'locality'] for item in loc_aux.split()] + zone_form + [\n",
        "                                        [item, 'locality'] for item in\n",
        "                                        loc_zone.split()]\n",
        "                                )\n",
        "                            else:\n",
        "                                components.append(\n",
        "                                    zone_form + [[item, 'locality'] for item in loc_zone.split()]\n",
        "                                    + locality_form + [[item, 'locality'] for item in loc_aux.split()]\n",
        "                                )\n",
        "                else:\n",
        "                    components.append(\n",
        "                        locality_form + [[item, 'locality'] for item in locality.split()]\n",
        "                    )\n",
        "            else:\n",
        "                components.append(\n",
        "                    locality_form + [[item, 'locality'] for item in locality.split()]\n",
        "                )\n",
        "\n",
        "        # CREATING COMPONENT 4  --- [municipality_form],[municipality] ---\n",
        "        if len(municipality) != 0 or not super().is_empty(municipality):\n",
        "            components.append(\n",
        "                municipality_prefix + [[item, 'municipality'] for item in municipality.split()]\n",
        "            )\n",
        "\n",
        "        # CREATING COMPONENT 5  --- [province_form],[province] ---\n",
        "        if len(province) != 0 or not super().is_empty(province):\n",
        "            components.append(\n",
        "                province_prefix + [[item, 'province'] for item in province.split()]\n",
        "            )\n",
        "\n",
        "            #  Permutación entre componentes.\n",
        "            components = super().generate_non_standardization(components, 50)\n",
        "\n",
        "\n",
        "        return components\n",
        "\n",
        "    def __generate_uncorrect_examples_type_two(self):\n",
        "        components = []\n",
        "\n",
        "        building = super().generate_building_syntetic()\n",
        "        locality = str(rm.choice(self.data['locality']))\n",
        "        municipality = str(rm.choice(self.data['municipality']))\n",
        "        province = str(rm.choice(self.data['province']))\n",
        "\n",
        "        building_form = super().generate_prefix_randomly(BUILDING_PREFIX, 55)\n",
        "        number_form = super().generate_prefix_randomly(PROPERTY_PREFIX, 55)\n",
        "        apartment_form = super().generate_prefix_randomly(APARTMENT_PREFIX, 55)\n",
        "        zone_form = super().generate_prefix_randomly(ZONE_PREFIX, 45)\n",
        "        locality_form = super().generate_prefix_randomly(LOCALITY_PREFIX, 45)\n",
        "        municipality_prefix = super().generate_prefix_randomly(MUNICIPALITY_PREFIX, 35)\n",
        "        province_prefix = super().generate_prefix_randomly(PROVINCE_PREFIX, 35)\n",
        "\n",
        "        # For creating the first component we have to split itself in three sub-components, if the name of the building is alphanum\n",
        "        # For example: Edif 456 o 34B\n",
        "        # [reserved word for building]+ [reserved word for number] + [building]\n",
        "        # then a random number is used to decide whether or not the number component appears\n",
        "        # And if it's alpha would be like:\n",
        "        # [reserved word for building] + [name]\n",
        "\n",
        "        is_name = True if building.isalpha() else False\n",
        "\n",
        "        # CREATING COMPONENT 1  --- [building_form],[number_form],[building] ---\n",
        "        if is_name:\n",
        "            components.append(\n",
        "                building_form + [[item, 'building'] for item in\n",
        "                                 building.split()]\n",
        "            )\n",
        "        else:\n",
        "            # Using the random number mentioned above\n",
        "            # Add property prefix\n",
        "            add_num_component = True if rm.randint(0, 100) > 50 else False\n",
        "            if add_num_component:\n",
        "                components.append(\n",
        "                    building_form + number_form + [[item, 'building'] for item in\n",
        "                                                   building.split()]\n",
        "                )\n",
        "            else:\n",
        "                components.append(\n",
        "                    building_form + [[item, 'building'] for item in\n",
        "                                     building.split()]\n",
        "                )\n",
        "\n",
        "        # CREATING COMPONENT 2  --- [apartment_form],[number_form],[apartment] ---\n",
        "        # This component is optional\n",
        "\n",
        "        add_apart_component = rm.randint(0, 100)\n",
        "        if add_apart_component > 50:\n",
        "            apartment_num = super().generate_apartment_syntetic()\n",
        "\n",
        "            # para variante num, letra+num\n",
        "            add_num_component = True if rm.randint(0, 100) > 50 else False\n",
        "            if add_num_component:\n",
        "                components.append(\n",
        "                    apartment_form + number_form + [[item, 'apartment'] for item in\n",
        "                                                    apartment_num.split()]\n",
        "                )\n",
        "            else:\n",
        "                components.append(\n",
        "                    apartment_form + [[item, 'apartment'] for item in\n",
        "                                      apartment_num.split()]\n",
        "                )\n",
        "\n",
        "        # CREATING COMPONENT 3  --- [locality_form],[locality] ---\n",
        "        if locality != '':\n",
        "            loc_aux = ''\n",
        "            loc_zone = ''\n",
        "            if locality.lower().find('alamar') != -1:\n",
        "                spl_loc = locality.split()\n",
        "                if len(spl_loc) != 1:\n",
        "                    loc_aux = 'Alamar'\n",
        "                    for word in spl_loc:\n",
        "                        if word.lower() != 'alamar':\n",
        "                            loc_zone += word + ' '\n",
        "\n",
        "                    # Adding the zone\n",
        "                    if loc_zone.lower().find('micro') != -1:\n",
        "                        if rm.randint(0, 50) > 25:\n",
        "                            components.append(\n",
        "                                locality_form + [[item, 'locality'] for item in loc_aux.split()] + zone_form + [\n",
        "                                    [item, 'locality'] for item in\n",
        "                                    loc_zone.split()]\n",
        "                            )\n",
        "                        else:\n",
        "                            components.append(\n",
        "                                zone_form + [[item, 'locality'] for item in loc_zone.split()]\n",
        "                                + locality_form + [[item, 'locality'] for item in loc_aux.split()]\n",
        "                            )\n",
        "\n",
        "                    else:\n",
        "                        add_num_component = True if rm.randint(0, 100) > 50 else False\n",
        "                        if add_num_component:\n",
        "                            if rm.randint(0, 50) > 25:\n",
        "                                components.append(\n",
        "                                    locality_form + [[item, 'locality'] for item in loc_aux.split()]\n",
        "                                    + zone_form + number_form + [[item, 'locality'] for item in\n",
        "                                                                 loc_zone.split()]\n",
        "                                )\n",
        "                            else:\n",
        "                                components.append(\n",
        "                                    zone_form + number_form + [[item, 'locality'] for item in loc_zone.split()]\n",
        "                                    + locality_form + [[item, 'locality'] for item in loc_aux.split()]\n",
        "                                )\n",
        "\n",
        "                        else:\n",
        "                            if rm.randint(0, 50) > 25:\n",
        "                                components.append(\n",
        "                                    locality_form + [[item, 'locality'] for item in loc_aux.split()] + zone_form + [\n",
        "                                        [item, 'locality'] for item in\n",
        "                                        loc_zone.split()]\n",
        "                                )\n",
        "                            else:\n",
        "                                components.append(\n",
        "                                    zone_form + [[item, 'locality'] for item in loc_zone.split()]\n",
        "                                    + locality_form + [[item, 'locality'] for item in loc_aux.split()]\n",
        "                                )\n",
        "                else:\n",
        "                    components.append(\n",
        "                        locality_form + [[item, 'locality'] for item in locality.split()]\n",
        "                    )\n",
        "            else:\n",
        "                components.append(\n",
        "                    locality_form + [[item, 'locality'] for item in locality.split()]\n",
        "                )\n",
        "\n",
        "        # CREATING COMPONENT 4  --- [municipality_form],[municipality] ---\n",
        "        if len(municipality) != 0 or not super().is_empty(municipality):\n",
        "            components.append(\n",
        "                municipality_prefix + [[item, 'municipality'] for item in municipality.split()]\n",
        "            )\n",
        "\n",
        "        # CREATING COMPONENT 5  --- [province_form],[province] ---\n",
        "        if len(province) != 0 or not super().is_empty(province):\n",
        "            components.append(\n",
        "                province_prefix + [[item, 'province'] for item in province.split()]\n",
        "            )\n",
        "\n",
        "        #  Permutación entre componentes.\n",
        "        components = super().generate_non_standardization(components, 50)\n",
        "\n",
        "        return components\n",
        "\n"
      ],
      "metadata": {
        "id": "gXznJccktrwC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras.optimizers\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import Sequential\n",
        "from keras.layers import LSTM, Embedding, Dense, Bidirectional, Concatenate, Reshape, Flatten\n",
        "from keras.layers import TextVectorization\n",
        "from tensorflow.python.ops.ragged.ragged_string_ops import string_bytes_split\n",
        "\n",
        "class DeepParserModel(NeuralParser):\n",
        "\n",
        "    def __init__(self, data_set: DataSet, cleaner_method, model=None):\n",
        "        self.data = data_set\n",
        "        self.cleaner_method = cleaner_method\n",
        "        if model is None:\n",
        "            output_dim = 100\n",
        "            input_length = 25\n",
        "\n",
        "            inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "\n",
        "            tv_by_character = self.__create_layer_vectorization(name='TextVectorization_Character',\n",
        "                                                                max_len=data_set.get_max_len_character(),\n",
        "                                                                split=string_bytes_split)\n",
        "            vocab_size_character = len(tv_by_character.get_layer('text_vectorization').get_vocabulary())\n",
        "            layer_tv_character = tv_by_character(inputs)\n",
        "\n",
        "            tv_by_trigram = self.__create_layer_vectorization(name='TextVectorization_Trigram',\n",
        "                                                              max_len=data_set.get_max_len_trigram(),\n",
        "                                                              split=string_bytes_split,\n",
        "                                                              ngrams=3)\n",
        "            vocab_size_trigram = len(tv_by_trigram.get_layer('text_vectorization_1').get_vocabulary())\n",
        "            layer_tv_by_trigram = tv_by_trigram(inputs)\n",
        "\n",
        "            tv_by_word = self.__create_layer_vectorization(name='TextVectorization_Word',\n",
        "                                                           max_len=data_set.get_max_len_word())\n",
        "            vocab_size_word = len(tv_by_word.get_layer('text_vectorization_2').get_vocabulary())\n",
        "            layer_tv_by_word = tv_by_word(inputs)\n",
        "\n",
        "            embedding_character = Embedding(vocab_size_character, 25, name='Embedding_Character')\n",
        "            layer_embedding_character = embedding_character(layer_tv_character)\n",
        "            blstm_character = Bidirectional(LSTM(units=25, return_sequences=True, dropout=0.6, recurrent_dropout=0.1),\n",
        "                                            merge_mode='concat')\n",
        "            layer_blstm_character = blstm_character(layer_embedding_character)\n",
        "\n",
        "            embedding_trigram = Embedding(vocab_size_trigram, 25, name='Embedding_Trigram')\n",
        "            layer_embedding_trigram = embedding_trigram(layer_tv_by_trigram)\n",
        "            blstm_trigram = Bidirectional(LSTM(units=25, return_sequences=True, dropout=0.6, recurrent_dropout=0.1),\n",
        "                                          merge_mode='concat')\n",
        "            layer_blstm_trigram = blstm_trigram(layer_embedding_trigram)\n",
        "\n",
        "            embedding_word = Embedding(vocab_size_word, output_dim, name='Embedding_Word')\n",
        "            layer_embedding_word = embedding_word(layer_tv_by_word)\n",
        "            concat = Concatenate()([layer_blstm_character, layer_blstm_trigram])\n",
        "            blstm_concat = Bidirectional(\n",
        "                LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.1),\n",
        "                merge_mode='concat')\n",
        "            layer_blstm_concat = blstm_concat(concat)\n",
        "\n",
        "            # ********* LAYER PROJECT ***************\n",
        "            projection = Flatten()(layer_blstm_concat)\n",
        "            projection = Dense(units=data_set.get_max_len_word() * 100)(projection)\n",
        "            projection = Reshape((data_set.get_max_len_word(), 100))(projection)\n",
        "            # ********* LAYER PROJECT ***************\n",
        "\n",
        "            concat_2 = Concatenate()([projection, layer_embedding_word])\n",
        "            blstm_concat_2 = Bidirectional(\n",
        "                LSTM(units=data_set.get_n_tag(), return_sequences=True, dropout=0, recurrent_dropout=0),\n",
        "                merge_mode='sum')\n",
        "            layer_blstm_concat_2 = blstm_concat_2(concat_2)\n",
        "\n",
        "            output = Dense(data_set.get_n_tag(), activation='softmax')(layer_blstm_concat_2)\n",
        "            model = keras.Model(inputs, output, name='Model')\n",
        "\n",
        "            # opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "            # Optimiser\n",
        "            opt = keras.optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999)\n",
        "            metrics = [tf.metrics.CategoricalAccuracy(), tf.metrics.Precision(), tf.metrics.Recall()]\n",
        "            # metrics = [tf.metrics.Accuracy()]\n",
        "            # Accuracy tells you how many times the ML model was correct overall.\n",
        "            # Precision is how good the model is at predicting a specific category.\n",
        "            # Recall tells you how many times the model was able to detect a specific category.\n",
        "\n",
        "            model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=metrics)\n",
        "            model.summary()\n",
        "            self.model = model\n",
        "            # plot_model(model, 'DeepParse_Architecture.jpg')\n",
        "        elif type(model) is not keras.engine.functional.Functional:\n",
        "            raise NotImplementedError('Model variable could be Keras.Model instance')\n",
        "        else:\n",
        "            self.model = model\n",
        "\n",
        "    def __create_layer_vectorization(self, name, max_len, ngrams=None, split=\"whitespace\"):\n",
        "        vectorize_layer = TextVectorization(\n",
        "            standardize=self.cleaner_method,\n",
        "            output_mode=\"int\",\n",
        "            output_sequence_length=max_len,\n",
        "            ngrams=ngrams,\n",
        "            split=split\n",
        "        )\n",
        "\n",
        "        vectorize_layer.adapt(self.get_data().get_vocabulary())\n",
        "\n",
        "        vectorize_layer_model = Sequential(name=name)\n",
        "        vectorize_layer_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "        vectorize_layer_model.add(vectorize_layer)\n",
        "\n",
        "        return vectorize_layer_model\n",
        "\n",
        "    def train(self, batch_size=1200, epochs=50):\n",
        "        x = np.asarray(self.data.get_x_train_sentence_values())\n",
        "        x_val = np.asarray(self.data.get_x_val_sentence_values())\n",
        "        print(self.data.get_y_train_values())\n",
        "        self.model.fit(x, self.data.get_y_train_values(), batch_size=batch_size,\n",
        "                       verbose=1, epochs=epochs, validation_data=(x_val, self.data.get_y_val_values()))\n",
        "\n",
        "    def predict(self, address_list: list):\n",
        "        print(self.data.get_id_to_category())\n",
        "        result = self.model.predict(address_list)\n",
        "\n",
        "        return np.round(result, decimals=2)\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.model.evaluate(\n",
        "            np.asarray(self.data.get_x_test_sentence_values()),\n",
        "            self.data.get_y_test_values())\n",
        "\n",
        "    def get_cleaner_method(self):\n",
        "        return self.cleaner_method\n",
        "\n",
        "    def get_data(self) -> DataSet:\n",
        "        return self.data\n",
        "\n",
        "    def get_model(self) -> keras.Model:\n",
        "        return self.model\n",
        "\n",
        "    def set_data(self, data: DataSet) -> None:\n",
        "        if data.get_n_tag() == self.data.get_n_tag():\n",
        "            self.data = data\n",
        "        else:\n",
        "            raise NotImplementedError('The number of tags does not correspond to the trained network')\n",
        "\n"
      ],
      "metadata": {
        "id": "Du1gQjzKtz0Q"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NeuralParser(ABC):\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, address_list: list):\n",
        "        pass\n",
        "    @abstractmethod\n",
        "    def train(self, batch_size=1200, epochs=50):\n",
        "        pass\n",
        "    @abstractmethod\n",
        "    def evaluate(self):\n",
        "        pass\n",
        "    @abstractmethod\n",
        "    def get_cleaner_method(self):\n",
        "        pass\n",
        "    @abstractmethod\n",
        "    def get_data(self) -> DataSet:\n",
        "        pass\n",
        "    @abstractmethod\n",
        "    def get_model(self) -> tf.keras.Model:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def set_data(self, data: DataSet) -> None:\n",
        "        pass\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GK7a5kpdt6IF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "class DataSet:\n",
        "    def __init__(self, vocabulary_word, max_len_character, max_len_trigram, max_len_word, n_tag, id_to_category, input_dim,\n",
        "                 x_train_sentence, x_test_sentence, x_val_sentence, y_train, y_test, y_val):\n",
        "        self.__vocabulary_word = vocabulary_word\n",
        "        self.max_len_character = max_len_character\n",
        "        self.max_len_trigram = max_len_trigram\n",
        "        self.max_len_word = max_len_word\n",
        "        self.__n_tag = n_tag\n",
        "        self.__id_to_category = id_to_category\n",
        "        self.__x_train_sentence = x_train_sentence\n",
        "        self.__x_test_sentence = x_test_sentence\n",
        "        self.__x_val_sentence = x_val_sentence\n",
        "        self.__y_train = y_train\n",
        "        self.__y_test = y_test\n",
        "        self.__y_val = y_val\n",
        "        self.__input_dim = input_dim\n",
        "\n",
        "    def get_x_train_sentence_values(self):\n",
        "        return self.__x_train_sentence\n",
        "\n",
        "    def get_x_test_sentence_values(self):\n",
        "        return self.__x_test_sentence\n",
        "\n",
        "    def get_x_val_sentence_values(self):\n",
        "        return self.__x_val_sentence\n",
        "\n",
        "    def get_y_train_values(self):\n",
        "        return self.__y_train\n",
        "\n",
        "    def get_y_test_values(self):\n",
        "        return self.__y_test\n",
        "\n",
        "    def get_y_val_values(self):\n",
        "        return self.__y_val\n",
        "\n",
        "    def get_vocabulary(self):\n",
        "        return self.__vocabulary_word\n",
        "\n",
        "    def get_max_len_character(self):\n",
        "        return self.max_len_character\n",
        "\n",
        "    def get_max_len_trigram(self):\n",
        "        return self.max_len_trigram\n",
        "\n",
        "    def get_max_len_word(self):\n",
        "        return self.max_len_word\n",
        "\n",
        "    def get_n_tag(self):\n",
        "        return self.__n_tag\n",
        "\n",
        "    def get_input_dim(self):\n",
        "        return self.__input_dim\n",
        "\n",
        "    def get_id_to_category(self):\n",
        "        return self.__id_to_category\n"
      ],
      "metadata": {
        "id": "eaO5VF8Nt-mB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STREET_NAME_PREFIX_CORRECT = ['calle', 'avenida',  'carretera',  'calzada',  'pasaje',  'callejon',  'callejuela',\n",
        "                      'acera',  'terraplen', 'camino']\n",
        "BETWEEN_PREFIX_CORRECT = ['entre', ]\n",
        "CORNER_CONECTOR_PREFIX_CORRECT = [ 'esquina']\n",
        "BUILDING_PREFIX_CORRECT = ['edificio', 'EDIFICIO', 'Edificio',]\n",
        "PROPERTY_PREFIX_CORRECT = [ 'número']\n",
        "DISTANCE_PREFIX_CORRECT = [ 'kilometro', 'Kilometro', 'KILOMETRO']\n",
        "DISTANCE_SPECIFICATION_PREFIX_CORRECT =['½','¼','¾','1/4','1/2','3/4']\n",
        "LOCALITY_PREFIX_CORRECT = ['pueblo', 'Pueblo', 'poblado', 'Poblado', 'caserio', 'Caserio',\n",
        "                   'reparto', 'Reparto', 'barrio',  'comunidad', 'Comunidad', 'distrito',  'Distrito', ]\n",
        "OTHER_PREFIX_CORRECT = ['batey', 'bat', 'ciudad','finca', 'Finca']\n",
        "PLACE_PREFIX_CORRECT = ['Bar','Club','Restaurante','Hotel','Centro comercial','Supermercado','Tienda minorista',\n",
        "                'Tienda mayorista','Mercado agropecuario','Bazar','Feria','Parque',\n",
        "                'Piscina','Zonas de escalada','Finca','Clínica','Hospital','Laboratorios']\n",
        "ZONE_PREFIX_CORRECT = [ 'zona',  'Zona']\n",
        "MUNICIPALITY_PREFIX_CORRECT = ['Municipio', 'municipio', 'MUNICIPIO']\n",
        "PROVINCE_PREFIX_CORRECT = [ 'Provincia', 'provicia', 'PROVINCIA', ]\n",
        "APARTMENT_PREFIX_CORRECT = [ 'apartamento', ]\n",
        "BUILDING_SUBDIVISION_PREFIX_CORRECT = ['apto', 'bloque', 'blq', 'esc', 'escalera', 'piso', 'apartamento', 'Bloque',\n",
        "                               ]\n",
        "CONJUNTION_CORRECT = ['y', 'e']\n",
        "\n",
        "STREET_NAME_PREFIX = ['calle', 'CALL', 'ave', 'avenida', 'ave.', 'Ave', 'Ave.', 'AVE', 'AVE.', 'carretera', 'ctra',\n",
        "                      'Ctra.', 'Ctra', 'carr',\n",
        "                      'Carr.', 'Carr', 'carret', 'Carret.', 'Carret', 'CARRET', 'CARRETE', 'calzada', 'czda.', 'calz',\n",
        "                      'Calzada', 'Czda',\n",
        "                      'Calz', 'czda.', 'calz.', 'Czda.', 'Calz.', 'pasaje', 'psje', 'callejon', 'cjon', 'callejuela',\n",
        "                      'acera', 'terraplan', 'terr', 'Terraplen', 'camino', 'calle', 'calle', 'calle', 'calle', 'calle', 'calle', 'calle', 'calle', 'calle']\n",
        "BETWEEN_PREFIX = ['e/', 'e/c', '%', 'entre', 'entre', 'entre', 'entre', 'entre', 'E\\\\', 'E/', 'ent.', 'etr.', 'e\\c', '/', '\\\\', 'e /']\n",
        "CORNER_CONNECTOR_PREFIX = ['esq', 'esquina']\n",
        "LOCALITY_PREFIX = ['pueblo', 'localidad', 'Pueblo', 'poblado', 'pob', 'Poblado', 'caserio', 'cas', 'csrio', 'Caserio',\n",
        "                   'batey', 'bat', 'ciudad','acera', 'terraplan', 'terr', 'Terraplen', 'camino']\n",
        "BUILDING_PREFIX = ['ed', 'edif', 'edf', 'edificio', 'EDIFICIO', 'Edificio', 'EDIF.', 'ED', 'e.','edf.', 'edi' 'EDF','Edif.', 'Edifi', 'edif.', 'ed.','']\n",
        "PROPERTY_PREFIX = ['#', 'no', 'S/n', 'S/N', 's/N', 's/n', 'nro.', 'nu', 'num', 'no.', 'num.', 'nu.', 'número', 'no','nro','']\n",
        "DISTANCE_PREFIX = ['Km.', 'KM.', 'Km', 'KM', 'K.', 'k.', 'kilometro', 'Kilometro', 'KILOMETRO', 'K\\M', 'K/M', 'k/m',\n",
        "                   'k\\m','kmts','kmts.',]\n",
        "DISTANCE_SPECIFICATION_PREFIX =['½','¼','¾','1/4','1/2','3/4']\n",
        "OTHER_PREFIX = ['batey', 'bat', 'ciudad','finca', 'Finca']\n",
        "PLACE_PREFIX = ['Bar','Club','Restaurante','Hotel','Centro comercial','Supermercado','Tienda minorista',\n",
        "                'Tienda mayorista','Mercado agropecuario','Bazar','Feria','Parque',\n",
        "                'Piscina','Zonas de escalada','Finca','Clínica','Hospital','Laboratorios']\n",
        "ZONE_PREFIX = ['', 'Zn.', 'zn.', 'zon.', 'z.', 'zona', 'zna.', 'za', 'zo.', 'Zona']\n",
        "MUNICIPALITY_PREFIX = ['Mun.', 'mun.', 'Mun', 'mun', 'Municipio', 'municipio', 'MUNICIPIO''M.','m.','mcpio.','Mno.', ' ']\n",
        "PROVINCE_PREFIX = ['Prov.', 'prov.', 'PROV.', 'Prov', 'prov', 'Provincia', 'provicia', 'PROVINCIA', 'Pro.', 'PRO.',\n",
        "                   'Pro', 'PRO', 'pro','prcia','provin.', 'prv.', '']\n",
        "APARTMENT_PREFIX = ['apart.', 'apt.', 'apto', 'apto.', 'apartamento', 'apt', 'ap', 'aptto', '']\n",
        "BUILDING_SUBDIVISION_PREFIX = ['apto', 'bloque', 'blq', 'esc', 'escalera', 'piso', 'Apto', 'apartamento', 'Bloque',\n",
        "                               'apto.', 'Apto.', 'apart', 'APTO', 'apt', 'apto']\n",
        "CONJUNCTION = ['y', 'e']\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zW4s7nBhuB-T"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "class DataSetManage:\n",
        "    @staticmethod\n",
        "    def save(data_set: DataSet, route_and_name: str):\n",
        "        if type(route_and_name) is not str:\n",
        "            raise NotImplementedError('route variable could be string instance')\n",
        "        with open(route_and_name + '.pickle', \"wb\") as file:\n",
        "            pickle.dump(data_set, file)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(route_and_name) -> DataSet:\n",
        "        data_set = None\n",
        "        with open(route_and_name + '.pickle', \"rb\") as file:\n",
        "            data_set = pickle.load(file)\n",
        "\n",
        "        if  type(route_and_name) is None:\n",
        "            raise NotImplementedError('route and name data set is not found')\n",
        "\n",
        "        return data_set\n",
        "\n",
        "    def export_data(df, filename, file_format='csv'):\n",
        "        if file_format == 'csv':\n",
        "            df.to_csv(filename + '.csv', index=False)\n",
        "        elif file_format == 'xlsx':\n",
        "            df.to_excel(filename + '.xlsx', index=False)"
      ],
      "metadata": {
        "id": "mH0KPszAuLTN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Decoder:\n",
        "\n",
        "    def __init__(self, id_to_cat: dict, cleaner_method):\n",
        "        self.cat_to_id = {v: k for k, v in id_to_cat.items()}\n",
        "        self.cleaner_method = cleaner_method\n",
        "\n",
        "    def decoder_to_first_address_model(self, matrix_probability, text_address_list):\n",
        "        list_address_classified = []\n",
        "        index_address = 0\n",
        "        for raw_address in matrix_probability:\n",
        "            components = {value: [] for key, value in self.cat_to_id.items()}\n",
        "\n",
        "            pre_presses_text = self.cleaner_method(text_address_list[index_address])\n",
        "            words = str(pre_presses_text.numpy().decode('utf-8')).split()\n",
        "\n",
        "            for i in range(len(words)):  # Lista de probabilidades\n",
        "                index_tag = list(raw_address[i]).index(max(list(raw_address[i])))\n",
        "                components[index_tag] += [words[i]]\n",
        "\n",
        "            principal_street = None\n",
        "            first_side_street = None\n",
        "            second_side_street = None\n",
        "            building = None\n",
        "            apartment = None\n",
        "            locality = None\n",
        "            municipality = None\n",
        "            province = None\n",
        "            reserve_word = None\n",
        "\n",
        "            for cat in self.cat_to_id:\n",
        "                if cat == 'principal_street':\n",
        "                    principal_street = components[self.cat_to_id[cat]]\n",
        "                elif cat == 'first_side_street':\n",
        "                    first_side_street = components[self.cat_to_id[cat]]\n",
        "                elif cat == 'second_side_street':\n",
        "                    second_side_street = components[self.cat_to_id[cat]]\n",
        "                elif cat == 'building':\n",
        "                    building = components[self.cat_to_id[cat]]\n",
        "                elif cat == 'apartment':\n",
        "                    apartment = components[self.cat_to_id[cat]]\n",
        "                elif cat == 'locality':\n",
        "                    locality = components[self.cat_to_id[cat]]\n",
        "                elif cat == 'municipality':\n",
        "                    municipality = components[self.cat_to_id[cat]]\n",
        "                elif cat == 'province':\n",
        "                    province = components[self.cat_to_id[cat]]\n",
        "                elif cat == 'rw':\n",
        "                    reserve_word = components[self.cat_to_id[cat]]\n",
        "\n",
        "            list_address_classified.append(\n",
        "                ClassifiedAddressOne(principal_street=principal_street, first_side_street=first_side_street, second_side_street=second_side_street,\n",
        "                                     locality=locality, municipality=municipality, province=province,\n",
        "                                     building=building, apartment=apartment, reserve_word=reserve_word))\n",
        "            index_address += 1\n",
        "\n",
        "        return list_address_classified\n",
        "\n",
        "    def decoder_to_second_address_model(self, matrix_probability, text_address_list):\n",
        "        list_address_classified = []\n",
        "        index_address = 0\n",
        "        for raw_address in matrix_probability:\n",
        "            components = {value: [] for key, value in self.cat_to_id.items()}\n",
        "\n",
        "            pre_presses_text = self.cleaner_method(text_address_list[index_address])\n",
        "            words = str(pre_presses_text.numpy().decode('utf-8')).split()\n",
        "\n",
        "            for i in range(len(words)):  # Lista de probabilidades\n",
        "                index_tag = list(raw_address[i]).index(max(list(raw_address[i])))\n",
        "                components[index_tag] += [words[i]]\n",
        "\n",
        "            building = None\n",
        "            apartment = None\n",
        "            locality = None\n",
        "            municipality = None\n",
        "            province = None\n",
        "            reserve_word = None\n",
        "\n",
        "            for cat in self.cat_to_id:\n",
        "                if cat == 'building':\n",
        "                    building = components[self.cat_to_id[cat]]\n",
        "                elif cat == 'apartment':\n",
        "                    apartment = components[self.cat_to_id[cat]]\n",
        "                elif cat == 'locality':\n",
        "                    locality = components[self.cat_to_id[cat]]\n",
        "                elif cat == 'municipality':\n",
        "                    municipality = components[self.cat_to_id[cat]]\n",
        "                elif cat == 'province':\n",
        "                    province = components[self.cat_to_id[cat]]\n",
        "                elif cat == 'rw':\n",
        "                    reserve_word = components[self.cat_to_id[cat]]\n",
        "\n",
        "            list_address_classified.append(\n",
        "                ClassifiedAddressTwo(locality, municipality,province, building, apartment, reserve_word))\n",
        "            index_address += 1\n",
        "\n",
        "        return list_address_classified"
      ],
      "metadata": {
        "id": "LQfqQxakuSVb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dill\n"
      ],
      "metadata": {
        "id": "H7ApE86vxnZX",
        "outputId": "483157e3-16c6-4c1f-ef09-688024dc50d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/110.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "Successfully installed dill-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import dill\n",
        "\n",
        "\n",
        "class NeuralParserManage:\n",
        "\n",
        "    @staticmethod\n",
        "    def save_neural_parser(neural_parser: NeuralParser, route='assets/trained_models/model_type_one/',\n",
        "                           name='model_pretrained'):\n",
        "        if type(route) is not str:\n",
        "            raise NotImplementedError('route variable could be string instance')\n",
        "        if type(name) is not str:\n",
        "            raise NotImplementedError('name_model variable could be string instance')\n",
        "        # save model\n",
        "        route = NeuralParserManage.__reformat_and_validate_route(route)\n",
        "        name = NeuralParserManage.__reformat_and_validate_name_model(name)\n",
        "        dir_model = route + '/' + name + '/model'\n",
        "        dir_data_set = route + '/' + name + '/data_set'\n",
        "        dir_cleaner_method = route + '/' + name + '/cleaner_method'\n",
        "\n",
        "        neural_parser.get_model().save(dir_model, save_format=\"tf\")\n",
        "        DataSetManage.save(neural_parser.get_data(), route_and_name=dir_data_set)\n",
        "\n",
        "        # Save cleaner method with dill\n",
        "        dill.dump(neural_parser.get_cleaner_method(), open(dir_cleaner_method, 'wb'))\n",
        "\n",
        "    @staticmethod\n",
        "    def load_neural_parser(route='default', name='default') -> NeuralParser:\n",
        "        file_path = route + '/' + name\n",
        "        # load data set\n",
        "        data = DataSetManage.load(file_path + '/data_set')\n",
        "\n",
        "        # load address cleaner\n",
        "        cleaner_method = dill.load(open(file_path + '/cleaner_method', 'rb'))\n",
        "        # load keras model\n",
        "        model = tf.keras.models.load_model(file_path + '/model', custom_objects={cleaner_method.__name__: cleaner_method, 'string_bytes_split': string_bytes_split})\n",
        "        return DeepParserModel(data, cleaner_method=cleaner_method, model=model)\n",
        "\n",
        "    @staticmethod\n",
        "    def __reformat_and_validate_route(route: str):\n",
        "        if len(route) == 0:\n",
        "            raise NotImplementedError('name_model variable cannot be an empty text')\n",
        "\n",
        "        if route[len(route) - 1] == '/':\n",
        "            route = route[:len(route) - 1]\n",
        "\n",
        "        if len(route.split()) == 0:\n",
        "            raise NotImplementedError('route variable cannot be an text with only withe space')\n",
        "\n",
        "        return route\n",
        "\n",
        "    @staticmethod\n",
        "    def __reformat_and_validate_name_model(name_model: str):\n",
        "        if len(name_model) == 0:\n",
        "            raise NotImplementedError('name_model variable cannot be an empty text')\n",
        "        if name_model[len(name_model) - 1] == '/':\n",
        "            name_model = name_model[:len(name_model) - 1]\n",
        "        if name_model[0] == '/':\n",
        "            name_model = name_model[1:]\n",
        "\n",
        "            # Repeating validation because the before steps has removed a character\n",
        "        if len(name_model) == 0:\n",
        "            raise NotImplementedError('name_model variable cannot be an empty text')\n",
        "        if len(name_model.split()) == 0:\n",
        "            raise NotImplementedError('name_model variable cannot be an text with only withe space')\n",
        "\n",
        "        return name_model\n"
      ],
      "metadata": {
        "id": "G8CyDk0BuWeH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Init')\n",
        "# # load corpus\n",
        "data = pd.read_excel('./corpus/corpus_short_havana.xlsx')\n",
        "generator = NoiseGeneratorModelTwo()\n",
        "data_with_noise = generator.generate_noise(data,type='eq', address_amount=20000)\n",
        "data_set = DataSetAdapter().adapt(data_with_noise, 0.80, 0.05, 0.15)\n",
        "DataSetManage.save(data_set, './dataset/EQ5_20000')\n",
        "\n"
      ],
      "metadata": {
        "id": "lbdxdvcayTdd",
        "outputId": "3b43e2e3-7885-407e-c7f1-c622c6c3559a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Init\n",
            "Generate_random_noise_type_two\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Loading Datasets')\n",
        "eq1_data = DataSetManage.load('./dataset/EQ1_20000')\n",
        "eq2_data = DataSetManage.load('./dataset/EQ2_20000')\n",
        "eq3_data = DataSetManage.load('./dataset/EQ3_20000')\n",
        "eq4_data = DataSetManage.load('./dataset/EQ4_20000')\n",
        "eq5_data = DataSetManage.load('./dataset/EQ5_20000')\n"
      ],
      "metadata": {
        "id": "KnYS8KKc1pe5",
        "outputId": "4202f217-1f63-43b2-a44b-cf5eea7d7ad5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Creating Neural Model')\n",
        "\n",
        "model = DeepParserModel(eq1_data, AddressCleaner.cleaner_method('custom_standardization'))\n",
        "\n",
        "print('Correct Examples Triaining')\n",
        "\n",
        "model.train(batch_size=1000, epochs=20)\n",
        "NeuralParserManage.save_neural_parser(model, route='./trained_models/',\n",
        "                                      name='MCT_T2_1000_20_20k')\n",
        "print('Saved MCT_T2_1000_20_20k ')"
      ],
      "metadata": {
        "id": "a4vBXky_2ORS",
        "outputId": "da905902-dde6-486e-9bb7-1c6bf6614a68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Neural Model\n",
            "Model: \"Model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " TextVectorization_Character (S  (None, 109)         0           ['input_1[0][0]']                \n",
            " equential)                                                                                       \n",
            "                                                                                                  \n",
            " TextVectorization_Trigram (Seq  (None, 109)         0           ['input_1[0][0]']                \n",
            " uential)                                                                                         \n",
            "                                                                                                  \n",
            " Embedding_Character (Embedding  (None, 109, 25)     1025        ['TextVectorization_Character[0][\n",
            " )                                                               0]']                             \n",
            "                                                                                                  \n",
            " Embedding_Trigram (Embedding)  (None, 109, 25)      208825      ['TextVectorization_Trigram[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 109, 50)      10200       ['Embedding_Character[0][0]']    \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, 109, 50)     10200       ['Embedding_Trigram[0][0]']      \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 109, 100)     0           ['bidirectional[0][0]',          \n",
            "                                                                  'bidirectional_1[0][0]']        \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirectional  (None, 109, 200)    160800      ['concatenate[0][0]']            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 21800)        0           ['bidirectional_2[0][0]']        \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 2000)         43602000    ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " TextVectorization_Word (Sequen  (None, 20)          0           ['input_1[0][0]']                \n",
            " tial)                                                                                            \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 20, 100)      0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " Embedding_Word (Embedding)     (None, 20, 100)      1731000     ['TextVectorization_Word[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 20, 200)      0           ['reshape[0][0]',                \n",
            "                                                                  'Embedding_Word[0][0]']         \n",
            "                                                                                                  \n",
            " bidirectional_3 (Bidirectional  (None, 20, 6)       9936        ['concatenate_1[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 20, 6)        42          ['bidirectional_3[0][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 45,734,028\n",
            "Trainable params: 45,734,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Correct Examples Triaining\n",
            "[[[0. 0. 0. 0. 0. 1.]\n",
            "  [0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 1.]\n",
            "  [0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 1.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. 0. 0. 1.]\n",
            "  [0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [0. 1. 0. 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 1.]]]\n",
            "Epoch 1/20\n",
            "16/16 [==============================] - 271s 15s/step - loss: 1.1029 - categorical_accuracy: 0.6307 - precision: 0.8514 - recall: 0.3774 - val_loss: 0.8771 - val_categorical_accuracy: 0.6683 - val_precision: 0.8769 - val_recall: 0.5307\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 233s 15s/step - loss: 0.8416 - categorical_accuracy: 0.6821 - precision: 0.8961 - recall: 0.5238 - val_loss: 0.8157 - val_categorical_accuracy: 0.6937 - val_precision: 0.9151 - val_recall: 0.5160\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 233s 15s/step - loss: 0.8005 - categorical_accuracy: 0.6976 - precision: 0.9147 - recall: 0.5245 - val_loss: 0.7782 - val_categorical_accuracy: 0.7093 - val_precision: 0.9104 - val_recall: 0.5357\n",
            "Epoch 4/20\n",
            " 3/16 [====>.........................] - ETA: 3:01 - loss: 0.7780 - categorical_accuracy: 0.7074 - precision: 0.9090 - recall: 0.5358"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Te damos la bienvenida a Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}